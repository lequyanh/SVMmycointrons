---
title: "Untitled"
author: "AnhVu"
date: "1/9/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Create grid table to match results from the search itself
cs <- factor(rep(c(100, 10, 1), each=12))
degs <- factor(rep(
    rep(c(15, 20, 25, 30), each=3),
  times=3))
win_ins <- factor(rep(c(60,70,80), times=12))
```

# Acceptor model
## Recall
```{r}
recalls <- read.table("data/gridsearch-acceptor-recalls.txt")
d <- recalls$V1

# Prepare table
recalls.df.flat <- data.frame(recall=d, C=cs, degree=degs, win=win_ins)
```

From the plots we see, that **recall prefers high regularization constant C (obviously) and low degree of kernels**
```{r}
par(mfrow=c(1,3))
boxplot(recall ~ C, data=recalls.df.flat)
boxplot(recall ~ degree, data=recalls.df.flat)
boxplot(recall ~ win, data=recalls.df.flat)

aov.out <- aov(recall ~ C, data=recalls.df.flat)
summary(aov.out)

aov.out <- aov(recall ~ degree, data=recalls.df.flat)
summary(aov.out)

aov.out <- aov(recall ~ win, data=recalls.df.flat)
summary(aov.out)
```

To compare results for $C=100$ and $C=1$ we can notice, that recall for all pairs of $degree$ and $window$ combinations, the SVMs with $C=1$ are worse.
```{r}
recalls.C100 <- recalls.df.flat[which(recalls.df.flat$C == 100),]
recalls.C1 <- recalls.df.flat[which(recalls.df.flat$C == 1),]
recalls.C100$recall - recalls.C1$recall
```

Conclusion: Results for $C=100$ and $C=10$ are the same. $C=1$ is worse in all aspects, regardles of $degree$ and $window$.
The size of window seem to increase recall, but not significantly (at the cost of higher computation time). The recall is basically influenced only by $C$ (as is also shown by ANOVA). 

## Precisions 
Precision optimal parametrization goes against the optimal parametrization of recall (it prefers **low C and high degree of kernel**):

```{r}
precisions <- read.table("data/gridsearch-acceptor-precisions.txt")
p <- precisions$V1
precisions.df.flat <- data.frame(precision=p, C=cs, degree=degs, win=win_ins)
```

Regularization constant doesn't seem to have an effect, even though we can see, that $C=1$ is somewhat better in precision (which is in contrary to recall findings, where $C=1$ was the worse). 
```{r}
par(mfrow = c(1,3))
boxplot(precision ~ C, data=precisions.df.flat)
boxplot(precision ~ degree, data=precisions.df.flat)
boxplot(precision ~ win, data=precisions.df.flat)

aov.out <- aov(precision ~ C, data=precisions.df.flat)
summary(aov.out)

aov.out <- aov(precision ~ degree, data=precisions.df.flat)
summary(aov.out)

aov.out <- aov(precision ~ win, data=precisions.df.flat)
summary(aov.out)
```

We see the effect of degree on precision is rather prominent in overall data, however it has no effect for e.g. $C=100$ 

```{r}
precisions.C100 <- precisions.df.flat[which(precisions.df.flat$C==100),]
aov.out <- aov(precision ~ win, data=precisions.C100)
summary(aov.out)
```

Finally we see the variance in recall and precision overall:

```{r}
boxplot(cbind(d, p))
```
The metrics seem to be equally spread. Therefore we will choose parameters to match the Ascomycota model for consistency. I.e. $C=10$ (to emphasise recall), $win=70$ (as window seem to slightly improve recall, no effect in precision) and $d=25$ (prominent beneficial effect on precision, negligible benefits in recall). We can consider increasing $d$ to 30. 

# Donor model
## Recall
```{r}
d_recalls <- read.table("data/gridsearch-donor-recalls.txt")
dr <- d_recalls$V1

# Prepare table
recalls.df.flat <- data.frame(recall=dr, C=cs, degree=degs, win=win_ins)
```

Similar results to acceptor site - **recall prefers high regularization constant C and low degree**. 
```{r}
par(mfrow=c(1,3))
boxplot(recall ~ C, data=recalls.df.flat)
boxplot(recall ~ degree, data=recalls.df.flat)
boxplot(recall ~ win, data=recalls.df.flat)

aov.out <- aov(recall ~ C, data=recalls.df.flat)
summary(aov.out)

aov.out <- aov(recall ~ degree, data=recalls.df.flat)
summary(aov.out)

aov.out <- aov(recall ~ win, data=recalls.df.flat)
summary(aov.out)
```

To compare results for $C=100$ and $C=1$ we can notice, that recall for all pairs of $degree$ and $window$ combinations, the SVMs with $C=1$ are again consistently worse.
```{r}
recalls.C1 <- recalls.df.flat[which(recalls.df.flat$C == 1),]
recalls.C100 <- recalls.df.flat[which(recalls.df.flat$C == 100),]
recalls.C100$recall - recalls.C1$recall
```

Conclusion: Results for $C=100$ and $C=10$ are the same. $C=1$ is worse in all aspects, regardles of $degree$ and $window$.
The size of the window nor the kernel degree seem to affect recall. 


## Precisions 
```{r}
precisions <- read.table("data/gridsearch-donor-precisions.txt")
dp <- precisions$V1
precisions.df.flat <- data.frame(precision=dp, C=cs, degree=degs, win=win_ins)
```

Regularization constant doesn't seem to have an effect, even though we can see, that $C=1$ is somewhat better in precision (which is in contrary to recall findings, where $C=1$ was the worse). 
```{r}
par(mfrow = c(1,3))
boxplot(precision ~ C, data=precisions.df.flat)
boxplot(precision ~ degree, data=precisions.df.flat)
boxplot(precision ~ win, data=precisions.df.flat)

aov.out <- aov(precision ~ C, data=precisions.df.flat)
summary(aov.out)

aov.out <- aov(precision ~ degree, data=precisions.df.flat)
summary(aov.out)

aov.out <- aov(precision ~ win, data=precisions.df.flat)
summary(aov.out)
```

Overall, we $C$ has the greatest effect and we again choose $C=10$ to emphasise recall, $d=25$ (for consistency with Ascomycota model; can be probably raised to 30 though). Window is set to 70 to emphasise recall (even though almost insignificantly)